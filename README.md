# Introduction to the gem5 emulator

This report is part of the assignment for the course *Computer Architecture and Accelerators* at the *Aristotle University of Thessaloniki*. The purpose of this assignment is to provide an introduction to the gem5 simulator, a widely used tool in computer architecture research.

## Part 1 - Introduction to gem5 and Hello World

### Question 1

The `starter_se.py` script configures a system for **syscall emulation (SE) mode** in gem5, where user-level applications are executed without a full operating system. The main parameters of the simulated system are summarized below.

#### **CPU Configuration**

- **CPU Model**: `MinorCPU`
- **Number of cores**: 1 (default value of `--num-cores`)
- **CPU frequency**: 1 GHz (default value of `--cpu-freq`)
- **Voltage**: 1.2 V

The CPU is created as part of a `CpuCluster`, which allows support for multi-core configurations, although only one core is used in this example.

#### **Cache hierarchy**

The cache hierarchy is enabled because the `MinorCPU` model operates in timing memory mode, which allows detailed modeling of cache and memory access latencies.

- **Cache line size**: 64 bytes
- **L1 Instruction Cache (L1I)**: enabled
- **L1 Data Cache (L1D)**: enabled
- **L2 Cache**: enabled and shared across the CPU cluster
- **TLB walk cache**: enabled

#### **Memory system**

- **Main memory type**: `DDR3_1600_8x8` (default value of `--mem-type`)
- **Memory size**: 2 GB (default value of `--mem-size`)
- **Memory channels**: 2 (default value of `--mem-channels`)
- **Memory bus**: SystemXBar (system crossbar interconnect)

The main memory is configured using the `MemConfig.config_mem()` helper function, which instantiates the selected DRAM model, connects it to the system bus, and maps it into the system’s physical address space.

#### **Clock and voltage**

- **System clock**: 1 GHz
- **System voltage**: 3.3 V

### Question 2

**a)** The `config.json` file generated by gem5 provides a detailed representation of the simulated system. By inspecting this file, we can verify the parameters identified in the previous question.

#### **CPU Configuration**

- **CPU Model**: `system > cpu_cluster > cpus > type`
- **Number of cores**: `system > cpu_cluster > cpus` (the list contains 1 object)
- **CPU frequency**: `system > cpu_cluster > clk_domain > clock`
- **Voltage**: `system > cpu_cluster > voltage_domain > voltage`

#### **Cache hierarchy**

- **Cache line size**: `system > cache_line_size`
- **L1 Instruction Cache (L1I)**: `system > cpu_cluster > cpus > icache`
- **L1 Data Cache (L1D)**: `system > cpu_cluster > cpus > dcache`
- **L2 Cache**: `system > cpu_cluster > l2`
- **TLB walk cache**: `system > cpu_cluster > cpus > itb_walker_cache`

#### **Memory system**

- **Main memory type**: `system > mem_ctrls > type`
- **Memory size**: `system > mem_ranges` (range from 0 to 2,147,483,647)
- **Memory channels**: `system > memories` (**2 memory controllers** in the list, each one manage a channel of RAM)
- **Memory bus**: `system > membus > type`

#### **Clock and voltage**

- **System clock**: `system > clk_domain > clock`
- **System voltage**: `system > voltage_domain > voltage`

**b)** The `stats.txt` file contains several metrics that describe the execution of the simulation.

- **sim_seconds**: Represents the total execution time of the program on the simulated system, measured in seconds. For the `hello-world` program, this is 0.000035 sec.
- **sim_ticks**: Indicates the total number of simulation ticks elapsed during execution. In gem5, one tick corresponds to one picosecond. For `hello-world` program, this is 35,452,000 ticks.
- **sim_insts**: The total number of instructions executed by the simulated CPU during the simulation. For `hello-world` program, this is 5,043 instructions.
- **host_inst_rate**: Describes the rate at which instructions are simulated on the host machine, reflecting the speed of the simulation rather than the performance of the simulated architecture. For `hello-world` program, this is 213,437 instruction per second.

**d)** In `stats.txt`, the total number of cache accesses can be found at:  

- **L1 Data Cache:** `system.cpu_cluster.cpus.dcache.overall_accesses::total`  
- **L2 Cache:** `system.cpu_cluster.l2.overall_accesses::total`  

For the `hello-world` program, the values are:  

- **L1 Data Cache accesses:** 2,185  
- **L2 Cache accesses:** 475  

If the total number of accesses is not directly reported, it can be calculated using the number of hits and misses:  

$$
\text{Total Accesses} = \text{Hits} + \text{Misses}
$$

**Examples:**  

- **L1 D-Cache:**  
  - Hits: 2,006 (`system.cpu_cluster.cpus.dcache.overall_hits::total`)  
  - Misses: 179 (`system.cpu_cluster.cpus.dcache.overall_misses::total`) 

- **L2 Cache:**  
  - Hits: 0
  - Misses: 475 (`system.cpu_cluster.l2.overall_misses::total`)

### Question 3

**In-order** processors execute instructions strictly in the sequential order defined by the compiled program, meaning the CPU must stall and wait if an instruction encounters a delay. In contrast, **out-of-order** processors can identify independent instructions and execute them ahead of stalled ones, reordering the workflow to hide latencies. The in-order processor models in gem5 are the following:

#### **AtomicSimpleCPU**

The **AtomicSimpleCPU** is the most basic model in gem5, designed primarily for extremely fast simulation speeds (e.g., for booting an operating system) rather than architectural accuracy. It uses **atomic memory accesses**, which means that memory requests are completed instantaneously in a single function call. The simulator calculates an approximate latency for the operation, but no simulation time actually passes during the access, and resource contention (such as a busy memory bus) is completely ignored.

#### **TimingSimpleCPU**

The **TimingSimpleCPU** is a non-pipelined model that serves as a middle ground, offering better accuracy than the atomic model by simulating the timing of memory interactions. It uses **timing memory accesses**, where memory requests are treated as realistic transactions that must travel through the cache hierarchy and interconnects. The CPU actually pauses execution until the memory system sends a response, and the simulator accurately models delays caused by resource contention, queues, and cache misses.

#### **MinorCPU**

The **MinorCPU** is a detailed, in-order processor model that is significantly more accurate than the SimpleCPU models because it simulates a fixed **4-stage pipeline** aad uses **timing memory accesses**. It accounts for pipeline hazards, bubbles, and data dependencies. The four stages of the pipeline function as follows:

- **Fetch1**: Fetches lines of data from the instruction cache.
- **Fetch2**: Breaks those lines into individual instructions and performs branch prediction.
- **Decode**: Decodes the instructions into micro-operations and prepares them for execution.
- **Execute**: Performs the arithmetic (ALU) operations, accesses memory (Load/Store), and commits the results to the architectural state.

**a)** The program [sum-1-t-n.c](./sum-1-to-n/sum-1-to-n.c) was executed in gem5 in syscall emulation mode with the same system parameters, but using two different CPU models:  **TimingSimpleCPU** and **MinorCPU**.

From the `stats.txt` files ([TimingSimpleCPU](./sum-1-to-n/TimingSimpleCPU/stats.txt), [MinorCPU](./sum-1-to-n/MinorCPU/stats.txt)) we obtain the following key simulation metrics:

#### **TimingSimpleCPU**

- `sim_insts` = 23,789
- `sim_ticks` = 1,834,846,000
- `sim_seconds` = 0.001835
- `system.cpu.numCycles` = 3,669,692

#### **MinorCPU**

- `sim_insts` = 23,845
- `sim_ticks` = 46,192,000
- `sim_seconds` = 0.000046
- `system.cpu.numCycles` = 92,384
- `system.cpu.ipc` = 0.258107
- `system.cpu.cpi` = 3.874355
- `system.cpu.idleCycles` = 58,013

We observe that, although the program executes approximately the same number of instructions on both models, the total simulation time (in ticks/seconds) is significantly smaller for MinorCPU compared to TimingSimpleCPU.

**b)** The measurements in the `stats.txt` files confirm the theoretical differences between **TimingSimpleCPU** and **MinorCPU**.

#### **Execution time / cycles / CPI**

For **TimingSimpleCPU**:
- `system.cpu.numCycles` = 3,669,692
- `sim_ticks` ≈ 1.8 × 10^9.
- `sim_insts` = 23,789
- $ \text{CPI} = \frac{\text{numCycles}}{\text{sim\_insts}} = \frac{3,669,692}{23,789} \approx 154$

For **MinorCPU**:
- `system.cpu.numCycles` = 92,384
- `sim_ticks` ≈ 4.6 × 10^7.
- `system.cpu.cpi` = 3.874355

Even though both models execute approximately the same number of instructions, MinorCPU completes the execution in **far fewer cycles and ticks**, which is consistent with the existence of a pipeline in MinorCPU. Moreover, the CPI (cycles per instruction) of TimingSimpleCPU is much higher, because even though both models use timing memory accesses—which force them to wait for memory operations to complete, as a real system does—MinorCPU can mitigate this disadvantage by overlapping work in the pipeline.

#### **Idle cycles**

For **TimingSimpleCPU**:
- `system.cpu.num_idle_cycles` = 0.002000

For **MinorCPU**:
- `system.cpu.idleCycles` = 58013

In TimingSimpleCPU, the core is modeled as a simple, non‑pipelined CPU that executes one instruction at a time and directly waits for each memory access to complete. These waits are counted as extra cycles and ticks, but they do not appear as idle time. In contrast, MinorCPU has an explicit 4‑stage pipeline, so stalls due to hazards, dependencies, bubbles, or branch behavior are visible as a large number of idle cycles, where parts of the pipeline are not doing useful work even though time is passing.

#### **Pipeline statistics**

For MinorCPU, additional statistics appear that do not exist for TimingSimpleCPU and are directly related to the detailed pipeline model:

- `system.cpu.fetch2.*` (e.g., `fetch2.int_instructions`, `fetch2.load_instructions`, `fetch2.store_instructions`)
  - statistics specific to the Fetch2 stage, confirming that the model has distinct pipeline stages.
- `system.cpu.branchPred.*` counters (BTB hits, conditional (in)correct, indirect hits/misses, etc.)
  - show the use of a branch prediction mechanism.
