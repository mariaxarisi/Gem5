# Introduction to the gem5 emulator

This report is part of the assignment for the course *Computer Architecture and Accelerators* at the *Aristotle University of Thessaloniki*. The purpose of this assignment is to provide an introduction to the gem5 simulator, a widely used tool in computer architecture research.

## Part 1 - Introduction to gem5 and Hello World

### Question 1

The [starter_se.py](./hello-world/starter_se.py) script configures a system for **syscall emulation (SE) mode** in gem5, where user-level applications are executed without a full operating system. The main parameters of the simulated system are summarized below.

#### **CPU Configuration**

- **CPU Model**: `MinorCPU`
- **Number of cores**: 1 (default value of `--num-cores`)
- **CPU frequency**: 1 GHz (default value of `--cpu-freq`)
- **Voltage**: 1.2 V

The CPU is created as part of a `CpuCluster`, which allows support for multi-core configurations, although only one core is used in this example.

#### **Cache hierarchy**

The cache hierarchy is enabled because the `MinorCPU` model operates in timing memory mode, which allows detailed modeling of cache and memory access latencies.

- **Cache line size**: 64 bytes
- **L1 Instruction Cache (L1I)**: enabled
- **L1 Data Cache (L1D)**: enabled
- **L2 Cache**: enabled and shared across the CPU cluster
- **TLB walk cache**: enabled

#### **Memory system**

- **Main memory type**: `DDR3_1600_8x8` (default value of `--mem-type`)
- **Memory size**: 2 GB (default value of `--mem-size`)
- **Memory channels**: 2 (default value of `--mem-channels`)
- **Memory bus**: SystemXBar (system crossbar interconnect)

The main memory is configured using the `MemConfig.config_mem()` helper function, which instantiates the selected DRAM model, connects it to the system bus, and maps it into the system’s physical address space.

#### **Clock and voltage**

- **System clock**: 1 GHz
- **System voltage**: 3.3 V

### Question 2

**a)** The [config.json](./hello-world/config.json) file generated by gem5 provides a detailed representation of the simulated system. By inspecting this file, we can verify the parameters identified in the previous question.

#### **CPU Configuration**

- **CPU Model**: `system > cpu_cluster > cpus > type`
- **Number of cores**: `system > cpu_cluster > cpus` (the list contains 1 object)
- **CPU frequency**: `system > cpu_cluster > clk_domain > clock`
- **Voltage**: `system > cpu_cluster > voltage_domain > voltage`

#### **Cache hierarchy**

- **Cache line size**: `system > cache_line_size`
- **L1 Instruction Cache (L1I)**: `system > cpu_cluster > cpus > icache`
- **L1 Data Cache (L1D)**: `system > cpu_cluster > cpus > dcache`
- **L2 Cache**: `system > cpu_cluster > l2`
- **TLB walk cache**: `system > cpu_cluster > cpus > itb_walker_cache`

#### **Memory system**

- **Main memory type**: `system > mem_ctrls > type`
- **Memory size**: `system > mem_ranges` (range from 0 to 2,147,483,647)
- **Memory channels**: `system > memories` (**2 memory controllers** in the list, each one manage a channel of RAM)
- **Memory bus**: `system > membus > type`

#### **Clock and voltage**

- **System clock**: `system > clk_domain > clock`
- **System voltage**: `system > voltage_domain > voltage`

**b)** The `stats.txt` file contains several metrics that describe the execution of the simulation.

- **sim_seconds**: Represents the total execution time of the program on the simulated system, measured in seconds. For the `hello-world` program, this is 0.000035 sec.
- **sim_ticks**: Indicates the total number of simulation ticks elapsed during execution. In gem5, one tick corresponds to one picosecond. For `hello-world` program, this is 35,452,000 ticks.
- **sim_insts**: The total number of instructions executed by the simulated CPU during the simulation. For `hello-world` program, this is 5,043 instructions.
- **host_inst_rate**: Describes the rate at which instructions are simulated on the host machine, reflecting the speed of the simulation rather than the performance of the simulated architecture. For `hello-world` program, this is 213,437 instruction per second.

**d)** In [stats.txt](./hello-world/stats.txt), the total number of cache accesses can be found at:  

- **L1 Data Cache:** `system.cpu_cluster.cpus.dcache.overall_accesses::total`  
- **L2 Cache:** `system.cpu_cluster.l2.overall_accesses::total`  

For the `hello-world` program, the values are:  

- **L1 Data Cache accesses:** 2,185  
- **L2 Cache accesses:** 475  

If the total number of accesses is not directly reported, it can be calculated using the number of hits and misses:  

$$
\text{Total Accesses} = \text{Hits} + \text{Misses}
$$

**Examples:**  

- **L1 D-Cache:**  
  - Hits: 2,006 (`system.cpu_cluster.cpus.dcache.overall_hits::total`)  
  - Misses: 179 (`system.cpu_cluster.cpus.dcache.overall_misses::total`) 

- **L2 Cache:**  
  - Hits: 0
  - Misses: 475 (`system.cpu_cluster.l2.overall_misses::total`)

### Question 3

**In-order** processors execute instructions strictly in the sequential order defined by the compiled program, meaning the CPU must stall and wait if an instruction encounters a delay. In contrast, **out-of-order** processors can identify independent instructions and execute them ahead of stalled ones, reordering the workflow to hide latencies. The in-order processor models in gem5 are the following:

#### **AtomicSimpleCPU**

The **AtomicSimpleCPU** is the most basic model in gem5, designed primarily for extremely fast simulation speeds (e.g., for booting an operating system) rather than architectural accuracy. It uses **atomic memory accesses**, which means that memory requests are completed instantaneously in a single function call. The simulator calculates an approximate latency for the operation, but no simulation time actually passes during the access, and resource contention (such as a busy memory bus) is completely ignored.

#### **TimingSimpleCPU**

The **TimingSimpleCPU** is a non-pipelined model that serves as a middle ground, offering better accuracy than the atomic model by simulating the timing of memory interactions. It uses **timing memory accesses**, where memory requests are treated as realistic transactions that must travel through the cache hierarchy and interconnects. The CPU actually pauses execution until the memory system sends a response, and the simulator accurately models delays caused by resource contention, queues, and cache misses.

#### **MinorCPU**

The **MinorCPU** is a detailed, in-order processor model that is significantly more accurate than the SimpleCPU models because it simulates a fixed **4-stage pipeline** aad uses **timing memory accesses**. It accounts for pipeline hazards, bubbles, and data dependencies. The four stages of the pipeline function as follows:

- **Fetch1**: Fetches lines of data from the instruction cache.
- **Fetch2**: Breaks those lines into individual instructions and performs branch prediction.
- **Decode**: Decodes the instructions into micro-operations and prepares them for execution.
- **Execute**: Performs the arithmetic (ALU) operations, accesses memory (Load/Store), and commits the results to the architectural state.

**a)** The program [sum-1-t-n.c](./sum-1-to-n/sum-1-to-n.c) was executed in gem5 in syscall emulation mode with the same system parameters, but using two different CPU models:  **TimingSimpleCPU** and **MinorCPU**.

From the `stats.txt` files ([TimingSimpleCPU](./sum-1-to-n/TimingSimpleCPU/2GHz-DDR3_1600_8x8/stats.txt), [MinorCPU](./sum-1-to-n/MinorCPU/2GHz-DDR3_1600_8x8/stats.txt)) we obtain the following key simulation metrics:

#### **TimingSimpleCPU**

- `sim_insts` = 23,789
- `sim_ticks` = 1,834,846,000
- `sim_seconds` = 0.001835
- `system.cpu.numCycles` = 3,669,692

#### **MinorCPU**

- `sim_insts` = 23,845
- `sim_ticks` = 46,192,000
- `sim_seconds` = 0.000046
- `system.cpu.numCycles` = 92,384
- `system.cpu.ipc` = 0.258107
- `system.cpu.cpi` = 3.874355
- `system.cpu.idleCycles` = 58,013

We observe that, although the program executes approximately the same number of instructions on both models, the total simulation time (in ticks/seconds) is significantly smaller for MinorCPU compared to TimingSimpleCPU.

**b)** The measurements in the `stats.txt` files confirm the theoretical differences between **TimingSimpleCPU** and **MinorCPU**.

#### **Execution time / cycles / CPI**

For **TimingSimpleCPU**:
- `system.cpu.numCycles` = 3,669,692
- `sim_ticks` ≈ 1.8 × 10^9.
- `sim_insts` = 23,789
- $ \text{CPI} = \frac{\text{numCycles}}{\text{sim\_insts}} = \frac{3,669,692}{23,789} \approx 154$

For **MinorCPU**:
- `system.cpu.numCycles` = 92,384
- `sim_ticks` ≈ 4.6 × 10^7.
- `system.cpu.cpi` = 3.874355

Even though both models execute approximately the same number of instructions, MinorCPU completes the execution in **far fewer cycles and ticks**, which is consistent with the existence of a pipeline in MinorCPU. Moreover, the CPI (cycles per instruction) of TimingSimpleCPU is much higher, because even though both models use timing memory accesses—which force them to wait for memory operations to complete, as a real system does—MinorCPU can mitigate this disadvantage by overlapping work in the pipeline.

#### **Idle cycles**

For **TimingSimpleCPU**:
- `system.cpu.num_idle_cycles` = 0.002000

For **MinorCPU**:
- `system.cpu.idleCycles` = 58013

In TimingSimpleCPU, the core is modeled as a simple, non‑pipelined CPU that executes one instruction at a time and directly waits for each memory access to complete. These waits are counted as extra cycles and ticks, but they do not appear as idle time. In contrast, MinorCPU has an explicit 4‑stage pipeline, so stalls due to hazards, dependencies, bubbles, or branch behavior are visible as a large number of idle cycles, where parts of the pipeline are not doing useful work even though time is passing.

#### **Pipeline statistics**

For MinorCPU, additional statistics appear that do not exist for TimingSimpleCPU and are directly related to the detailed pipeline model:

- `system.cpu.fetch2.*` (e.g., `fetch2.int_instructions`, `fetch2.load_instructions`, `fetch2.store_instructions`)
  - statistics specific to the Fetch2 stage, confirming that the model has distinct pipeline stages.
- `system.cpu.branchPred.*` counters (BTB hits, conditional (in)correct, indirect hits/misses, etc.)
  - show the use of a branch prediction mechanism.

**c)** The results that we mentioned above were derived for 2 GHz CPU frequency and a `DDR3_1600_8x8` memory type. In order to observe how the change in CPU frequency affects the simulation results for both **TimingSimpleCPU** and **MinorCPU**, we simulated the same program with a 1 GHz CPU clock as well.

#### **TimingSimpleCPU**

- **Total simulated time and ticks**
  - **1 GHz**
    - `sim_ticks`: 1 831 972 000  
    - `sim_seconds`: 0.001832 → ≈ 1.832 µs  
  - **2 GHz**
    - `sim_ticks`: 1 834 846 000  
    - `sim_seconds`: 0.001835 → ≈ 1.835 µs  
  - **Observation:** The simulated time is practically the same; the 2 GHz run is only very slightly slower. Increasing the CPU frequency does not give a meaningful speedup in simulated time.

- **CPU cycles and busy/idle fractions**
  - **1 GHz**
    - `system.cpu.numCycles`: 1 831 972  
    - `system.cpu.num_busy_cycles`: 1 831 971.999  
    - `system.cpu.idle_fraction`: 0.0  
    - `system.cpu.not_idle_fraction`: 1.0  
  - **2 GHz**
    - `system.cpu.numCycles`: 3 669 692  
    - `system.cpu.num_busy_cycles`: 3 669 691.998  
    - `system.cpu.idle_fraction`: 0.0  
    - `system.cpu.not_idle_fraction`: 1.0  
  - **Observation:** The 2 GHz run uses almost exactly 2× the number of CPU cycles, while both runs are essentially busy for all cycles (no significant idle time). We are doing the same work in almost the same simulated time, but accounting it in about twice as many cycles at 2 GHz.

**Explanation:** For TimingSimpleCPU, doubling the CPU frequency does not significantly change the total execution time in seconds, because the model is non‑pipelined: it executes one instruction at a time and explicitly waits for memory operations to complete. Memory and DRAM latencies are defined in absolute time (ns / ticks), so when we double the CPU frequency the absolute memory latency stays the same, but it is represented by more CPU cycles. As a result, `numCycles` almost doubles while `sim_seconds` remains almost constant.

#### **MinorCPU**

- **Total simulated time and ticks**
  - **1 GHz**
    - `sim_ticks`: 61 110 000  
    - `sim_seconds`: 0.000061 → ≈ 61 ns  
  - **2 GHz**
    - `sim_ticks`: 46 192 000  
    - `sim_seconds`: 0.000046 → ≈ 46 ns  
  - **Observation:** The 2 GHz configuration finishes with fewer ticks and in less simulated time.

- **Cycles, CPI, IPC**
  - **1 GHz**
    - `system.cpu.numCycles`: 61 110  
    - `system.cpu.cpi`: 2.566676  
    - `system.cpu.ipc`: 0.389609  
  - **2 GHz**
    - `system.cpu.numCycles`: 92 384  
    - `system.cpu.cpi`: 3.874355  
    - `system.cpu.ipc`: 0.258107  
  - **Observation:** The 2 GHz run uses more CPU cycles to execute almost the same number of instructions. CPI increases significantly and IPC decreases.

- **Idle cycles**
  - **1 GHz**
    - `system.cpu.idleCycles`: 27 056  
  - **2 GHz**
    - `system.cpu.idleCycles`: 58 013  
  - **Observation:** Idle cycles double at 2 GHz. This means that the extra cycles in the 2 GHz run are mostly stall/idle cycles.

**Explanation:** For MinorCPU, which is an in‑order pipelined core with timing memory accesses, doubling the CPU frequency does reduce the total simulated time, but it also makes the core more sensitive to memory latency in terms of cycles. Memory stalls are defined in absolute time, so when the CPU clock is faster, each fixed memory latency spans more CPU cycles. This causes the number of cycles and CPI to increase and IPC to decrease, and we see many more idle cycles where the pipeline is stalled waiting for data. Overall, the 2 GHz MinorCPU run completes the program faster in simulated seconds, but it does so with worse CPI/IPC and a higher fraction of stall cycles, showing that the core becomes more memory‑bound when the CPU frequency increases without a matching improvement in the memory

Keeping the CPU frequency at 2 GHz, we simulated the system with a DDR3_2133_8x8 memory type as well.

#### **TimingSimpleCPU**

- **Total simulated time and ticks**
  - **DDR3_1600_8x8**
    - `sim_ticks`: 1 834 846 000  
    - `sim_seconds`: 0.001835  
  - **DDR3_2133_8x8**
    - `sim_ticks`: 1 759 221 000  
    - `sim_seconds`: 0.001759  
  - **Observation:** With the faster DDR3_2133_8x8 memory, total simulated time decreases from ≈1.835 ms to ≈1.759 ms.

- **Instructions and cycles**
  - **DDR3_1600_8x8**
    - `sim_insts`: 23 789  
    - `system.cpu.numCycles`: 3 669 692  
  - **DDR3_2133_8x8**
    - `sim_insts`: 23 753  
    - `system.cpu.numCycles`: 3 518 442  
  - **Observation:** The number of executed instructions is almost the same, but total CPU cycles decrease by ~4% with DDR3_2133_8x8.

**Explanation:** For TimingSimpleCPU at 2 GHz, switching from DDR3_1600_8x8 to the faster DDR3_2133_8x8 reduces both the number of CPU cycles and the total simulated time by a few percent. Since this model is non‑pipelined and blocks on each memory access, any reduction in main‑memory latency translates directly into shorter stalls and fewer cycles. However, because this workload is relatively small, the overall speedup is modest (~4%), showing that memory is important but not the only factor limiting performance.

#### **MinorCPU**

- **Total simulated time and ticks**
  - **DDR3_1600_8x8**
    - `sim_ticks`: 46 192 000  
    - `sim_seconds`: 0.000046  
  - **DDR3_2133_8x8**
    - `sim_ticks`: 44 909 000  
    - `sim_seconds`: 0.000045  
  - **Observation:** With DDR3_2133_8x8 the execution time drops from ≈46 ns to ≈45 ns.

- **Cycles, CPI, IPC, idle cycles**
  - **DDR3_1600_8x8**
    - `system.cpu.numCycles`: 92 384  
    - `system.cpu.cpi`: 3.874355  
    - `system.cpu.ipc`: 0.258107  
    - `system.cpu.idleCycles`: 58 013  
  - **DDR3_2133_8x8**
    - `system.cpu.numCycles`: 89 820  
    - `system.cpu.cpi`: 3.772439  
    - `system.cpu.ipc`: 0.265080  
    - `system.cpu.idleCycles`: 55 516  
  - **Observation:** With the faster memory, total cycles, CPI, and idle cycles all decrease slightly, while IPC increases slightly.

**Explanation:** For MinorCPU at 2 GHz, moving from DDR3_1600_8x8 to DDR3_2133_8x8 shortens main‑memory latency, which reduces the time the in‑order pipeline spends stalled on memory. This appears in the stats as fewer total cycles, a lower CPI, fewer idle cycles, and a slightly higher IPC, along with a small reduction in simulated time. Compared to TimingSimpleCPU, MinorCPU can overlap some work in its pipeline, so it is a bit less directly exposed to memory latency; nevertheless, both models show that a faster DRAM technology yields a small but measurable improvement in performance.

## Part 2 - Execute SPEC CPU2006 benchmarks in gem5

For this exercise the following benchmarks were used from the **SPEC CPU2006** suite:

- [401.bzip2](https://www.spec.org/cpu2006/Docs/401.bzip2.html)
- [429.mcf](https://www.spec.org/cpu2006/Docs/429.mcf.html)
- [456.hmmer](https://www.spec.org/cpu2006/Docs/456.hmmer.html)
- [458.sjeng](https://www.spec.org/cpu2006/Docs/458.sjeng.html)
- [470.lbm](https://www.spec.org/cpu2006/Docs/470.lbm.html)

### Question 1

All simulations for the five benchmarks were executed on the same simulated system. Using the configuration in [config.ini](./spec/specbzip/config.ini), we derived the following hardware parameters:

#### **L1 Instruction Cache**

- **Section**: `[system.cpu.icache]`
- **Size (capacity)**: `size=32768` → **32 KB**
- **Associativity**: `assoc=2` → **2‑way set associative**

#### **L1 Data Cache**

- **Section**: `[system.cpu.dcache]`
- **Size (capacity)**: `size=65536` → **64 KB**
- **Associativity**: `assoc=2` → **2‑way set associative**

#### **L2 Cache**

- **Section**: `[system.l2]`
- **Size (capacity)**: `size=2097152` → **2 MB**
- **Associativity**: `assoc=8` → **8‑way set associative**
#### **Cache Line Size**

- **Section**: `[system]`
- **Cache line size**: `cache_line_size=64` → **64 bytes**

#### **Main Memory Capacity**

- **Section**: `[system]`
- **Main memory capacity**: `mem_ranges=0:536870911` → **512 MB**

### Question 2

To compare the behaviour of the five SPEC benchmarks on this configuration, we collected the following metrics from the `stats.txt` files and plotted them:

- **Committed instructions** (`system.cpu.committedInsts`): ![Committed instructions](./assets/benchmark-comparison/committed_insts.png)
- **CPI** (`system.cpu.cpi`): ![CPI per benchmark](./assets/benchmark-comparison/cpi.png)
- **L1 D‑cache overall miss rate** (`system.cpu.dcache.overall_miss_rate::total`): ![L1D miss rate](./assets/benchmark-comparison/l1d_miss_rate.png)
- **L1 I‑cache overall miss rate** (`system.cpu.icache.overall_miss_rate::total`): ![L1I miss rate](./assets/benchmark-comparison/l1i_miss_rate.png)
- **L2 cache overall miss rate** (`system.l2.overall_miss_rate::total`): ![L2 miss rate](./assets/benchmark-comparison/l2_miss_rate.png)
- **Simulated time** (`sim_seconds`): ![Execution time](./assets/benchmark-comparison/sim_seconds.png)

Because the simulations were run with the `-I 100000000` flag, all benchmarks were stopped after **100M committed instructions**. The equal bars in the first plot therefore confirm that every benchmark executed the same instruction budget; differences in performance come from how efficiently each benchmark uses the pipeline and memory system.

- **401.bzip2 (specbzip)** – Bzip2 shows relatively low CPI and low L1D/L2 miss rates, so for the same 100M instructions it finishes quickly. Its compression loops have decent spatial locality, and the pipeline is rarely stalled, making it one of the most efficient integer workloads in this set.

- **429.mcf (specmcf)** – MCF has a CPI similar to bzip2 but a noticeable L1I miss rate, reflecting a larger and more complex code footprint. However, its L1D and L2 miss rates remain low, so most data accesses are served from the caches and the overall execution time stays close to bzip2.

- **456.hmmer (spechmmer)** – HMMER achieves the lowest CPI among all benchmarks together with very low miss rates at every cache level. Its dynamic‑programming style loops have excellent locality, so the CPU spends most cycles doing useful work and it is the fastest benchmark under the fixed 100M‑instruction limit.

- **470.lbm (speclibm)** – LBM has a much higher CPI and medium L1D miss rate, while its L2 miss rate is close to 1.0. This indicates a streaming, bandwidth‑intensive workload that frequently goes to main memory; the long‑latency memory accesses combined with expensive floating‑point operations explain its significantly higher CPI and longer simulated time.

- **458.sjeng (specsjeng)** – Sjeng exhibits the highest CPI and the worst L1D and L2 miss rates, while its I‑cache behaviour is good. As a chess engine with irregular data structures and deep, branch‑heavy search, it generates many cache misses and stalls even though instruction locality is good. Consequently, the pipeline is often waiting on data, and sjeng has by far the largest simulated time among the five benchmarks.